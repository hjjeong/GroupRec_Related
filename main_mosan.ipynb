{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#Python Libs\n",
    "import numpy as np\n",
    "from time import time\n",
    "import datetime\n",
    "import gc\n",
    "#Implementations\n",
    "from model.mosan import MoSAN\n",
    "from config_douban import Config\n",
    "from dataset_Meetup import GDataset\n",
    "from batch_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, epoch_id, config):\n",
    "    # user trainning\n",
    "    t1 = time()\n",
    "    model.train()\n",
    "    learning_rates = config.lr\n",
    "    lr = learning_rates[0]\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    *_, last = train_loader\n",
    "\n",
    "    for batch_id, (g, pi_ni) in enumerate(train_loader):\n",
    "        # Data Load\n",
    "        group_input = g\n",
    "        pos_item_input = pi_ni[:, 0]\n",
    "        neg_item_input = pi_ni[:, 1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        pos_prediction = model(group_input, pos_item_input)\n",
    "        neg_prediction = model(group_input, neg_item_input)\n",
    "        #print(pos_prediction[0], neg_prediction[0])\n",
    "\n",
    "        # Loss\n",
    "\n",
    "        loss = torch.mean(-F.logsigmoid(pos_prediction-neg_prediction))\n",
    "        \n",
    "\n",
    "        \n",
    "        # record loss history\n",
    "        #print(\"batch_id: \" + str(batch_id) + \" loss: \" + str(loss.item()))\n",
    "        if not torch.isinf(loss.data) and not torch.isnan(loss.data):\n",
    "            losses.append(float(loss.item()))\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del group_input, pos_item_input, neg_item_input\n",
    "    gc.collect()\n",
    "    print('Iteration %d,\\tloss: [%.4f], time: [%.1fs]' % (epoch_id, np.mean(np.array(losses)), time() - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_model(model, test_groups, Ks, group_trainRatings, num_items):\n",
    "    valid_group = []\n",
    "    valid_item = []\n",
    "    for g in test_groups:\n",
    "        for v in test_groups[g]:\n",
    "            valid_group.append(g)\n",
    "            valid_item.append(v)\n",
    "\n",
    "            \n",
    "    group_var = torch.LongTensor(valid_group).cuda()\n",
    "    item_var = torch.LongTensor(valid_item).cuda()\n",
    "\n",
    "    model.eval()\n",
    "    rating = model(group_var, item_var)\n",
    "    avg_rating = torch.mean(rating).item()\n",
    "    return avg_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, groups_to_test, Ks, trainRatings, num_items):\n",
    "    model.eval()\n",
    "    t2 = time()\n",
    "    ret = test(model, groups_to_test, Ks, trainRatings, num_items) #See batch_test\n",
    "\n",
    "    print('\\t Evaluation done [%.1f s]' % (time() - t2))\n",
    "    for i, k in enumerate(Ks):\n",
    "        print('\\t\\t @%d: HR = %.4f, NDCG = %.4f, Rec = %.4f' % (k, ret['hit_ratio'][i], ret['ndcg'][i], ret['recall'][i]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "douban 127 256 [0.0005, 1e-06, 5e-07]\n"
     ]
    }
   ],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_groups: 92489 num_users: 63893 num_items: 15741\n"
     ]
    }
   ],
   "source": [
    "dataset = GDataset(config.user_dataset, config.group_dataset, config.user_in_group_path, 1)\n",
    "num_groups, num_users, num_items = dataset.num_groups, dataset.num_users, dataset.num_items\n",
    "print(\"num_groups: \"+str(num_groups)+\" num_users: \"+str(num_users)+\" num_items: \"+str(num_items))\n",
    "gu_dict = dataset.gu_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_warm = dataset.load_rating_file_as_dict(config.group_dataset + \".test.rating_warm\")\n",
    "test_cold = dataset.load_rating_file_as_dict(config.group_dataset + \".test.rating_cold\")\n",
    "valid = dataset.load_rating_file_as_dict(config.group_dataset + \".valid.rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoSAN: embedding size 32, run Iteration: 127, #neg: 4\n",
      "Iteration 0,\tloss: [0.5273], time: [1970.1s]\n",
      "\t Evaluation done [1.6 s]: 1.060\n",
      "saved\n",
      "Iteration 1,\tloss: [0.3877], time: [1975.1s]\n",
      "Iteration 2,\tloss: [0.3374], time: [1978.7s]\n",
      "Iteration 3,\tloss: [0.3139], time: [1980.0s]\n",
      "\t Evaluation done [1.5 s]: 2.158\n",
      "saved\n",
      "Iteration 4,\tloss: [0.2991], time: [1984.9s]\n",
      "Iteration 5,\tloss: [0.2902], time: [1988.0s]\n",
      "Iteration 6,\tloss: [0.2836], time: [1983.4s]\n",
      "\t Evaluation done [1.6 s]: 2.480\n",
      "saved\n",
      "Iteration 7,\tloss: [0.2801], time: [1983.0s]\n",
      "Iteration 8,\tloss: [0.2755], time: [1983.2s]\n",
      "Iteration 9,\tloss: [0.2711], time: [1977.2s]\n",
      "\t Evaluation done [1.5 s]: 2.657\n",
      "saved\n",
      "Iteration 10,\tloss: [0.2705], time: [1972.7s]\n",
      "Iteration 11,\tloss: [0.2674], time: [1972.0s]\n",
      "Iteration 12,\tloss: [0.2661], time: [1982.7s]\n",
      "\t Evaluation done [1.5 s]: 2.664\n",
      "saved\n",
      "Iteration 13,\tloss: [0.2628], time: [1971.3s]\n",
      "Iteration 14,\tloss: [0.2635], time: [1974.9s]\n",
      "Iteration 15,\tloss: [0.2629], time: [1971.2s]\n",
      "\t Evaluation done [1.5 s]: 2.693\n",
      "saved\n",
      "Iteration 16,\tloss: [0.2621], time: [1967.6s]\n",
      "Iteration 17,\tloss: [0.2626], time: [1975.6s]\n",
      "Iteration 18,\tloss: [0.2594], time: [1973.6s]\n",
      "\t Evaluation done [1.5 s]: 2.735\n",
      "saved\n",
      "Iteration 19,\tloss: [0.2592], time: [1975.7s]\n",
      "Iteration 20,\tloss: [0.2589], time: [1979.1s]\n",
      "Iteration 21,\tloss: [0.2594], time: [1972.5s]\n",
      "\t Evaluation done [1.5 s]: 2.696\n",
      "Iteration 22,\tloss: [0.2584], time: [1970.2s]\n",
      "Iteration 23,\tloss: [0.2574], time: [1983.9s]\n",
      "Iteration 24,\tloss: [0.2574], time: [1971.6s]\n",
      "\t Evaluation done [1.5 s]: 2.742\n",
      "saved\n",
      "Iteration 25,\tloss: [0.2564], time: [1970.4s]\n",
      "Iteration 26,\tloss: [0.2563], time: [1974.0s]\n",
      "Iteration 27,\tloss: [0.2564], time: [1973.6s]\n",
      "\t Evaluation done [1.5 s]: 2.724\n",
      "Iteration 28,\tloss: [0.2575], time: [1977.6s]\n",
      "Iteration 29,\tloss: [0.2541], time: [1973.7s]\n",
      "Iteration 30,\tloss: [0.2578], time: [1970.9s]\n",
      "\t Evaluation done [1.5 s]: 2.662\n",
      "Iteration 31,\tloss: [0.2569], time: [1974.8s]\n",
      "Iteration 32,\tloss: [0.2559], time: [1977.5s]\n",
      "Iteration 33,\tloss: [0.2550], time: [1974.5s]\n",
      "\t Evaluation done [1.5 s]: 2.770\n",
      "saved\n",
      "Iteration 34,\tloss: [0.2552], time: [1976.5s]\n",
      "Iteration 35,\tloss: [0.2565], time: [1974.5s]\n",
      "Iteration 36,\tloss: [0.2539], time: [1975.3s]\n",
      "\t Evaluation done [1.6 s]: 2.772\n",
      "saved\n",
      "Iteration 37,\tloss: [0.2534], time: [1971.6s]\n",
      "Iteration 38,\tloss: [0.2540], time: [1971.0s]\n",
      "Iteration 39,\tloss: [0.2564], time: [1981.5s]\n",
      "\t Evaluation done [1.5 s]: 2.761\n",
      "Iteration 40,\tloss: [0.2555], time: [1973.3s]\n",
      "Iteration 41,\tloss: [0.2549], time: [1970.2s]\n",
      "Iteration 42,\tloss: [0.2556], time: [1974.9s]\n",
      "\t Evaluation done [1.5 s]: 2.745\n",
      "Iteration 43,\tloss: [0.2551], time: [1972.7s]\n",
      "Iteration 44,\tloss: [0.2536], time: [1971.8s]\n",
      "Iteration 45,\tloss: [0.2556], time: [1974.1s]\n",
      "\t Evaluation done [1.5 s]: 2.746\n",
      "Iteration 46,\tloss: [0.2542], time: [1970.5s]\n",
      "Iteration 47,\tloss: [0.2534], time: [1988.5s]\n",
      "Iteration 48,\tloss: [0.2549], time: [1978.1s]\n",
      "\t Evaluation done [1.6 s]: 2.753\n",
      "Iteration 49,\tloss: [0.2543], time: [1987.5s]\n",
      "Iteration 50,\tloss: [0.2546], time: [1977.2s]\n",
      "Iteration 51,\tloss: [0.2546], time: [1981.0s]\n",
      "\t Evaluation done [1.5 s]: 2.792\n",
      "saved\n",
      "Iteration 52,\tloss: [0.2541], time: [1975.9s]\n",
      "Iteration 53,\tloss: [0.2539], time: [1974.8s]\n",
      "Iteration 54,\tloss: [0.2539], time: [1971.5s]\n",
      "\t Evaluation done [1.5 s]: 2.776\n",
      "Iteration 55,\tloss: [0.2546], time: [1981.9s]\n",
      "Iteration 56,\tloss: [0.2530], time: [1986.3s]\n",
      "Iteration 57,\tloss: [0.2547], time: [1980.9s]\n",
      "\t Evaluation done [1.4 s]: 2.746\n",
      "Iteration 58,\tloss: [0.2542], time: [1974.2s]\n",
      "Iteration 59,\tloss: [0.2529], time: [1981.3s]\n",
      "Iteration 60,\tloss: [0.2524], time: [1980.4s]\n",
      "\t Evaluation done [1.4 s]: 2.746\n",
      "Iteration 61,\tloss: [0.2544], time: [1978.9s]\n",
      "Iteration 62,\tloss: [0.2534], time: [1978.4s]\n",
      "Iteration 63,\tloss: [0.2508], time: [1973.0s]\n",
      "\t Evaluation done [1.6 s]: 2.787\n",
      "Iteration 64,\tloss: [0.2534], time: [1980.5s]\n",
      "Iteration 65,\tloss: [0.2527], time: [1975.2s]\n",
      "Iteration 66,\tloss: [0.2524], time: [1972.7s]\n",
      "\t Evaluation done [1.5 s]: 2.817\n",
      "saved\n",
      "Iteration 67,\tloss: [0.2530], time: [1974.6s]\n",
      "Iteration 68,\tloss: [0.2542], time: [1974.3s]\n",
      "Iteration 69,\tloss: [0.2530], time: [1974.6s]\n",
      "\t Evaluation done [1.5 s]: 2.805\n",
      "Iteration 70,\tloss: [0.2540], time: [1975.1s]\n",
      "Iteration 71,\tloss: [0.2528], time: [1970.0s]\n",
      "Iteration 72,\tloss: [0.2542], time: [1974.7s]\n",
      "\t Evaluation done [1.5 s]: 2.789\n",
      "Iteration 73,\tloss: [0.2543], time: [1974.7s]\n",
      "Iteration 74,\tloss: [0.2519], time: [1980.0s]\n",
      "Iteration 75,\tloss: [0.2531], time: [1975.9s]\n",
      "\t Evaluation done [1.5 s]: 2.802\n",
      "Iteration 76,\tloss: [0.2534], time: [1976.7s]\n",
      "Iteration 77,\tloss: [0.2548], time: [1967.8s]\n",
      "Iteration 78,\tloss: [0.2529], time: [1973.7s]\n",
      "\t Evaluation done [1.6 s]: 2.807\n",
      "Iteration 79,\tloss: [0.2519], time: [1970.1s]\n",
      "Iteration 80,\tloss: [0.2543], time: [1969.1s]\n",
      "Iteration 81,\tloss: [0.2536], time: [1974.7s]\n",
      "\t Evaluation done [1.5 s]: 2.709\n",
      "Iteration 82,\tloss: [0.2542], time: [1973.1s]\n",
      "Iteration 83,\tloss: [0.2539], time: [1978.4s]\n",
      "Iteration 84,\tloss: [0.2534], time: [1978.1s]\n",
      "\t Evaluation done [1.5 s]: 2.792\n",
      "Iteration 85,\tloss: [0.2532], time: [1974.7s]\n",
      "Iteration 86,\tloss: [0.2539], time: [1984.2s]\n",
      "Iteration 87,\tloss: [0.2547], time: [1973.2s]\n",
      "\t Evaluation done [1.5 s]: 2.773\n",
      "Iteration 88,\tloss: [0.2533], time: [1978.4s]\n",
      "Iteration 89,\tloss: [0.2536], time: [1987.5s]\n",
      "Iteration 90,\tloss: [0.2535], time: [1986.4s]\n",
      "\t Evaluation done [1.3 s]: 2.800\n",
      "Iteration 91,\tloss: [0.2531], time: [1979.2s]\n",
      "Iteration 92,\tloss: [0.2525], time: [1971.7s]\n",
      "Iteration 93,\tloss: [0.2529], time: [1975.9s]\n",
      "\t Evaluation done [1.5 s]: 2.757\n",
      "Iteration 94,\tloss: [0.2529], time: [1975.9s]\n",
      "Iteration 95,\tloss: [0.2521], time: [1974.8s]\n",
      "Iteration 96,\tloss: [0.2538], time: [1976.9s]\n",
      "\t Evaluation done [1.5 s]: 2.776\n",
      "Iteration 97,\tloss: [0.2529], time: [1969.3s]\n",
      "Iteration 98,\tloss: [0.2531], time: [1979.7s]\n",
      "Iteration 99,\tloss: [0.2541], time: [1977.3s]\n",
      "\t Evaluation done [1.4 s]: 2.780\n",
      "Iteration 100,\tloss: [0.2533], time: [1977.2s]\n",
      "Iteration 101,\tloss: [0.2533], time: [1976.8s]\n",
      "Iteration 102,\tloss: [0.2519], time: [1970.9s]\n",
      "\t Evaluation done [1.5 s]: 2.812\n",
      "Iteration 103,\tloss: [0.2521], time: [1972.3s]\n",
      "Iteration 104,\tloss: [0.2520], time: [1979.8s]\n",
      "Iteration 105,\tloss: [0.2522], time: [1970.3s]\n",
      "\t Evaluation done [1.4 s]: 2.787\n",
      "Iteration 106,\tloss: [0.2531], time: [1974.9s]\n",
      "Iteration 107,\tloss: [0.2543], time: [1972.1s]\n",
      "Iteration 108,\tloss: [0.2525], time: [1982.5s]\n",
      "\t Evaluation done [1.5 s]: 2.805\n",
      "Iteration 109,\tloss: [0.2519], time: [1971.4s]\n",
      "Iteration 110,\tloss: [0.2524], time: [1973.9s]\n",
      "Iteration 111,\tloss: [0.2520], time: [1980.0s]\n",
      "\t Evaluation done [1.4 s]: 2.837\n",
      "saved\n",
      "Iteration 112,\tloss: [0.2532], time: [1973.5s]\n",
      "Iteration 113,\tloss: [0.2517], time: [1974.8s]\n",
      "Iteration 114,\tloss: [0.2529], time: [1971.9s]\n",
      "\t Evaluation done [1.5 s]: 2.812\n",
      "Iteration 115,\tloss: [0.2526], time: [1975.9s]\n",
      "Iteration 116,\tloss: [0.2532], time: [1972.7s]\n",
      "Iteration 117,\tloss: [0.2538], time: [1974.1s]\n",
      "\t Evaluation done [1.5 s]: 2.793\n",
      "Iteration 118,\tloss: [0.2520], time: [1975.8s]\n",
      "Iteration 119,\tloss: [0.2527], time: [1972.7s]\n",
      "Iteration 120,\tloss: [0.2513], time: [1972.5s]\n",
      "\t Evaluation done [1.5 s]: 2.878\n",
      "saved\n",
      "Iteration 121,\tloss: [0.2525], time: [1981.3s]\n",
      "Iteration 122,\tloss: [0.2518], time: [1976.4s]\n",
      "Iteration 123,\tloss: [0.2522], time: [1971.0s]\n",
      "\t Evaluation done [1.5 s]: 2.795\n",
      "Iteration 124,\tloss: [0.2512], time: [1980.0s]\n",
      "Iteration 125,\tloss: [0.2508], time: [1971.7s]\n",
      "Iteration 126,\tloss: [0.2530], time: [1941.9s]\n",
      "\t Evaluation done [1.4 s]: 2.778\n"
     ]
    }
   ],
   "source": [
    "agree = MoSAN(num_users, num_items, num_groups, config.embedding_size, gu_dict, 0.5).cuda()\n",
    "best_checkpoint = -1.0\n",
    "best_weights_path = None\n",
    "for num_negatives in config.num_negatives:\n",
    "    dataset.num_negatives = num_negatives\n",
    "    print(\"MoSAN: embedding size %d, run Iteration: %d, #neg: %d\" %(config.embedding_size, config.epoch, num_negatives))\n",
    "    # train the model\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    for epoch in range(config.epoch):\n",
    "        training(agree, dataset.get_group_dataloader(config.batch_size), epoch, config)\n",
    "        \n",
    "\n",
    "        # Evaluation\n",
    "        if epoch % 3 == 0:\n",
    "            agree.eval()\n",
    "            with torch.no_grad():\n",
    "                t2 = time()\n",
    "                ret = valid_model(agree, test_warm, config.topK[:2], dataset.group_trainRatings, dataset.num_items)\n",
    "                cur_checkpoint = ret\n",
    "                print('\\t Evaluation done [%.1f s]: %.3f' % (time() - t2,ret))\n",
    "                #cur_checkpoint = ret['hit_ratio'][1]\n",
    "                #ret = evaluation(agree, test_cold, config.topK[:2], dataset.group_trainRatings, dataset.num_items)\n",
    "                #cur_checkpoint += ret['hit_ratio'][1]\n",
    "                #cur_checkpoint = cur_checkpoint/2\n",
    "                current_weights_path = 'weights/MoSAN'+str(config.dataset)+\"_\"+str(config.embedding_size)+\"_\"+str(config.lr[0])+'_'+str(num_negatives)+'_'+str(epoch)\n",
    "                torch.save(agree.state_dict(), current_weights_path)\n",
    "                if best_checkpoint <= cur_checkpoint:\n",
    "                    print('saved')\n",
    "                    best_weights_path = current_weights_path\n",
    "                    best_checkpoint = cur_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/MoSANdouban_32_0.0005_4_120\n",
      "cold\n",
      "\t Evaluation done [115661.8 s]\n",
      "\t\t @5: HR = 0.0121, NDCG = 0.0072, Rec = 0.0121\n",
      "\t\t @10: HR = 0.0227, NDCG = 0.0105, Rec = 0.0227\n",
      "\t Evaluation done [22990.8 s]\n",
      "\t\t @5: HR = 0.0143, NDCG = 0.0085, Rec = 0.0143\n",
      "\t\t @10: HR = 0.0221, NDCG = 0.0109, Rec = 0.0221\n"
     ]
    }
   ],
   "source": [
    "print(best_weights_path)\n",
    "agree = MoSAN(num_users, num_items, num_groups, config.embedding_size, gu_dict, config.drop_ratio).cuda()\n",
    "gc.collect()\n",
    "agree.load_state_dict(torch.load(best_weights_path))\n",
    "print('cold')\n",
    "ret = evaluation(agree, test_cold, config.topK[:2], dataset.group_trainRatings, dataset.num_items)\n",
    "\n",
    "ret = evaluation(agree, test_warm, config.topK[:2], dataset.group_trainRatings, dataset.num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
